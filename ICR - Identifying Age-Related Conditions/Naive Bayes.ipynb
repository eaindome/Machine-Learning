{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1e2635a",
   "metadata": {},
   "source": [
    "train.csv - The training set.\n",
    "Id Unique identifier for each observation.\n",
    "AB-GL Fifty-six anonymized health characteristics. All are numeric except for EJ, which is categorical.\n",
    "Class A binary target: 1 indicates the subject has been diagnosed with one of the three conditions, 0 indicates they have not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac707430",
   "metadata": {},
   "source": [
    "[Logistic Regression, Random Forest, Gradient Boosting Models (e.g., XGBoost, LightGBM, Support Vector Machines (SVM)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4042a6eb",
   "metadata": {},
   "source": [
    "GridSearchCV\n",
    "RandomizedSearchCV\n",
    "Bayesian Optimization\n",
    "HyperOpt\n",
    "Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006a429e",
   "metadata": {},
   "source": [
    "##### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d420924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96a7286",
   "metadata": {},
   "source": [
    "###### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815517f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f102e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaab7491",
   "metadata": {},
   "source": [
    "##### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84bb3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['EJ'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2da4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting categorical columns\n",
    "data = data.replace({'EJ':{'A':0, 'B':1}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51b25c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6257b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing age values with the median value\n",
    "data['BQ'] = data['BQ'].fillna(data['BQ'].median())\n",
    "data['CB'] = data['CB'].fillna(data['CB'].median())\n",
    "data['CC'] = data['CC'].fillna(data['CC'].median())\n",
    "data['DU'] = data['DU'].fillna(data['DU'].median())\n",
    "data['EL'] = data['EL'].fillna(data['EL'].median())\n",
    "\n",
    "data['FC'] = data['FC'].fillna(data['FC'].median())\n",
    "data['FL'] = data['FL'].fillna(data['FL'].median())\n",
    "data['FS'] = data['FS'].fillna(data['FS'].median())\n",
    "data['GL'] = data['GL'].fillna(data['GL'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8119f380",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e44dec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38af55e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bf54f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5096328",
   "metadata": {},
   "source": [
    "###### Find correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb792bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = data.corr()\n",
    "\n",
    "correlation_threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644e1b13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find highly correlated features\n",
    "highly_correlated_features = correlation_matrix[((correlation_matrix > correlation_threshold) | (correlation_matrix < -correlation_threshold)) & (correlation_matrix != 1)]\n",
    "highly_correlated_features = highly_correlated_features.unstack().dropna().reset_index()\n",
    "\n",
    "# Print the highly correlated features\n",
    "print(\"Highly Correlated Features:\")\n",
    "for index, row in highly_correlated_features.iterrows():\n",
    "    feature1 = row['level_0']\n",
    "    feature2 = row['level_1']\n",
    "    correlation = row[0]\n",
    "    print(f\"{feature1} - {feature2}: {correlation:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4382832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find highly correlated features with diagnosis\n",
    "highly_correlated_features = np.abs(correlation_matrix['Class']).sort_values(ascending=False)\n",
    "highly_correlated_features= highly_correlated_features[highly_correlated_features > correlation_threshold]\n",
    "\n",
    "# print the highly correlated features\n",
    "print(\"Highly Correlated Features with Diagnosis: \")\n",
    "for feature, correlation in highly_correlated_features.iteritems():\n",
    "    print(f\"{feature}: {correlation:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fd3ae3",
   "metadata": {},
   "source": [
    "## Data training and prediction analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f67aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve, auc, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b1ccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['Class', 'Id'], axis=1)\n",
    "y = data['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90c7c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4a95d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adf1912",
   "metadata": {},
   "source": [
    "###### Cross-validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd0c9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to evaluate the model using cross-validation and print various metrics\n",
    "def evaluate_model(model, X_train, y_train):\n",
    "    cv_accuracy = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    cv_precision = cross_val_score(model, X_train, y_train, cv=5, scoring='precision')\n",
    "    cv_recall = cross_val_score(model, X_train, y_train, cv=5, scoring='recall')\n",
    "    cv_f1 = cross_val_score(model, X_train, y_train, cv=5, scoring='f1')\n",
    "    cv_roc_auc = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "\n",
    "    print(f\"Cross-Validation Accuracy: {np.mean(cv_accuracy)*100:.2f}\")\n",
    "    print(f\"Cross-Validation Precision: {np.mean(cv_precision)*100:.2f}\")\n",
    "    print(f\"Cross-Validation Recall: {np.mean(cv_recall)*100:.2f}\")\n",
    "    print(f\"Cross-Validation F1-Score: {np.mean(cv_f1)*100:.2f}\")\n",
    "    print(f\"Cross-Validation ROC-AUC: {np.mean(cv_roc_auc)*100:.2f}\")\n",
    "\n",
    "    return cv_accuracy, cv_precision, cv_recall, cv_f1, cv_roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e37955",
   "metadata": {},
   "source": [
    "### Without Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0eff47",
   "metadata": {},
   "source": [
    "#### Unscaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6986912",
   "metadata": {},
   "source": [
    "##### Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582bd73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy*100}\")\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a27c938",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the learning curves for the best model\n",
    "plt.figure(figsize=(8, 6))\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    estimator=model, X=X_train, y=y_train, train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    cv=5, scoring='accuracy', shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "# Calculate the mean and standard deviation of train and validation scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# Plot the learning curves with the mean and standard deviation\n",
    "plt.plot(train_sizes, train_scores_mean, label='Training Accuracy', color='blue')\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color='blue')\n",
    "plt.plot(train_sizes, val_scores_mean, label='Validation Accuracy', color='red')\n",
    "plt.fill_between(train_sizes, val_scores_mean - val_scores_std, val_scores_mean + val_scores_std, alpha=0.2, color='red')\n",
    "\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Learning Curves')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75c758d",
   "metadata": {},
   "source": [
    "##### Test 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fe24d7",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beadc5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {'var_smoothing': np.logspace(0, -9, num=100)}\n",
    "\n",
    "# Create a Naive Bayes classifier\n",
    "model = GaussianNB()\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and its parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Make predictions on the test data using the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# print the best parameters found during hyperparameter tuning\n",
    "print(\"\\nBest Parameters: \")\n",
    "print(best_params)\n",
    "\n",
    "# evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy*100}\")\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0c8aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves for the best model\n",
    "plt.figure(figsize=(8, 6))\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    estimator=best_model, X=X_train, y=y_train, train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    cv=5, scoring='accuracy', shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "# Calculate the mean and standard deviation of train and validation scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# Plot the learning curves with the mean and standard deviation\n",
    "plt.plot(train_sizes, train_scores_mean, label='Training Accuracy', color='blue')\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color='blue')\n",
    "plt.plot(train_sizes, val_scores_mean, label='Validation Accuracy', color='red')\n",
    "plt.fill_between(train_sizes, val_scores_mean - val_scores_std, val_scores_mean + val_scores_std, alpha=0.2, color='red')\n",
    "\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Learning Curves')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a887d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d405a334",
   "metadata": {},
   "source": [
    "#### Scaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017b1e50",
   "metadata": {},
   "source": [
    "##### Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2e9622",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy*100}\")\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8963b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves for the best model\n",
    "plt.figure(figsize=(8, 6))\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    estimator=model, X=X_train_scaled, y=y_train, train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    cv=5, scoring='accuracy', shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "# Calculate the mean and standard deviation of train and validation scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# Plot the learning curves with the mean and standard deviation\n",
    "plt.plot(train_sizes, train_scores_mean, label='Training Accuracy', color='blue')\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color='blue')\n",
    "plt.plot(train_sizes, val_scores_mean, label='Validation Accuracy', color='red')\n",
    "plt.fill_between(train_sizes, val_scores_mean - val_scores_std, val_scores_mean + val_scores_std, alpha=0.2, color='red')\n",
    "\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Learning Curves')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f357c1",
   "metadata": {},
   "source": [
    "##### Test 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790a083e",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d341f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {'var_smoothing': np.logspace(0, -9, num=100)}\n",
    "\n",
    "# Create a Naive Bayes classifier\n",
    "model = GaussianNB()\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best model and its parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Make predictions on the test data using the best model\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "# print the best parameters found during hyperparameter tuning\n",
    "print(\"\\nBest Parameters: \")\n",
    "print(best_params)\n",
    "\n",
    "print(f\"Accuracy: {accuracy*100}\")\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a37062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves for the best model\n",
    "plt.figure(figsize=(8, 6))\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    estimator=best_model, X=X_train_scaled, y=y_train, train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    cv=5, scoring='accuracy', shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "# Calculate the mean and standard deviation of train and validation scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# Plot the learning curves with the mean and standard deviation\n",
    "plt.plot(train_sizes, train_scores_mean, label='Training Accuracy', color='blue')\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color='blue')\n",
    "plt.plot(train_sizes, val_scores_mean, label='Validation Accuracy', color='red')\n",
    "plt.fill_between(train_sizes, val_scores_mean - val_scores_std, val_scores_mean + val_scores_std, alpha=0.2, color='red')\n",
    "\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Learning Curves')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec04744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfe362ba",
   "metadata": {},
   "source": [
    "#### With cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c84f847",
   "metadata": {},
   "source": [
    "#### Unscaled-data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bea5310",
   "metadata": {},
   "source": [
    "##### Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab3590b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Logistic Regression without scaling\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using cross-validation and print various metrics\n",
    "print(\"Test 1: Support Vector Machines without scaling\")\n",
    "cv_accuracy, cv_precision, cv_recall, cv_f1, cv_roc_auc = evaluate_model(model, X_train, y_train)\n",
    "\n",
    "# Plot the precision-recall curve\n",
    "y_probs = model.decision_function(X_test)\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_probs)\n",
    "plt.plot(recall, precision, color='b', label='Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f31196",
   "metadata": {},
   "source": [
    "##### Test 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c63e48",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565d7847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {'var_smoothing': np.logspace(0, -9, num=100)}\n",
    "\n",
    "# Create a Naive Bayes classifier\n",
    "model = GaussianNB()\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and its parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Make predictions on the test data using the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# print the best parameters found during hyperparameter tuning\n",
    "print(\"\\nBest Parameters: \")\n",
    "print(best_params)\n",
    "\n",
    "# Evaluate the model using cross-validation and print various metrics\n",
    "print(\"Test 1: Gaussian Naive Bayes without scaling\")\n",
    "cv_accuracy, cv_precision, cv_recall, cv_f1, cv_roc_auc = evaluate_model(best_model, X_train, y_train)\n",
    "\n",
    "# Plot the precision-recall curve\n",
    "y_probs = model.decision_function(X_test)\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_probs)\n",
    "plt.plot(recall, precision, color='b', label='Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a97e046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves for the best model\n",
    "plt.figure(figsize=(8, 6))\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    estimator=model, X=X_train, y=y_train, train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    cv=5, scoring='accuracy', shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "# Calculate the mean and standard deviation of train and validation scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# Plot the learning curves with the mean and standard deviation\n",
    "plt.plot(train_sizes, train_scores_mean, label='Training Accuracy', color='blue')\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color='blue')\n",
    "plt.plot(train_sizes, val_scores_mean, label='Validation Accuracy', color='red')\n",
    "plt.fill_between(train_sizes, val_scores_mean - val_scores_std, val_scores_mean + val_scores_std, alpha=0.2, color='red')\n",
    "\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Learning Curves')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1564de6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5729d79f",
   "metadata": {},
   "source": [
    "#### Scaled-data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc0b9f1",
   "metadata": {},
   "source": [
    "##### Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0223259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Logistic Regression with scaling\n",
    "model = GaussianNB()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model using cross-validation and print various metrics\n",
    "print(\"Test 2: Support Vector Machines with scaling\")\n",
    "cv_accuracy, cv_precision, cv_recall, cv_f1, cv_roc_auc = evaluate_model(model, X_train_scaled, y_train)\n",
    "\n",
    "# Plot the precision-recall curve\n",
    "y_probs = model.decision_function(X_test_scaled)\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_probs)\n",
    "plt.plot(recall, precision, color='b', label='Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9369b4c2",
   "metadata": {},
   "source": [
    "##### Test 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad78f23",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb97ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {'var_smoothing': np.logspace(0, -9, num=100)}\n",
    "\n",
    "# Create a Naive Bayes classifier\n",
    "model = GaussianNB()\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best model and its parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Make predictions on the test data using the best model\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "# print the best parameters found during hyperparameter tuning\n",
    "print(\"\\nBest Parameters: \")\n",
    "print(best_params)\n",
    "\n",
    "# Evaluate the model using cross-validation and print various metrics\n",
    "print(\"Test 2: Gaussian Naive Bayes with scaling\")\n",
    "cv_accuracy, cv_precision, cv_recall, cv_f1, cv_roc_auc = evaluate_model(best_model, X_train_scaled, y_train)\n",
    "\n",
    "# Plot the precision-recall curve\n",
    "y_probs = model.decision_function(X_test_scaled)\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_probs)\n",
    "plt.plot(recall, precision, color='b', label='Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1933d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves for the best model\n",
    "plt.figure(figsize=(8, 6))\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    estimator=model, X=X_train_scaled, y=y_train, train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    cv=5, scoring='accuracy', shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "# Calculate the mean and standard deviation of train and validation scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# Plot the learning curves with the mean and standard deviation\n",
    "plt.plot(train_sizes, train_scores_mean, label='Training Accuracy', color='blue')\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color='blue')\n",
    "plt.plot(train_sizes, val_scores_mean, label='Validation Accuracy', color='red')\n",
    "plt.fill_between(train_sizes, val_scores_mean - val_scores_std, val_scores_mean + val_scores_std, alpha=0.2, color='red')\n",
    "\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Learning Curves')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e55ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
