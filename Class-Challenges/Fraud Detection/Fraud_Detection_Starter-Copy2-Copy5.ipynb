{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqLQ-hbyb5jH"
   },
   "source": [
    "# Fraud Detection in Electricity and Gas Consumption Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKfpS4UVJ4Wj"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OJMD_I3EJWP4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exEGyf6vJ_4T"
   },
   "source": [
    "## Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "I2oIx7IuKCJm"
   },
   "outputs": [],
   "source": [
    "client_train = pd.read_csv('client_train.csv', low_memory=False)\n",
    "invoice_train = pd.read_csv('invoice_train.csv', low_memory=False)\n",
    "\n",
    "client_test = pd.read_csv('client_test.csv', low_memory=False)\n",
    "invoice_test = pd.read_csv('invoice_test.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-LOrxdIU7-h"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "aVypp4ahU9tq"
   },
   "outputs": [],
   "source": [
    "#convert the column invoice_date to date time format on both the invoice train and invoice test\n",
    "for df in [invoice_train,invoice_test]:\n",
    "    df['invoice_date'] = pd.to_datetime(df['invoice_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "sW2W5i63VYyK"
   },
   "outputs": [],
   "source": [
    "#encode labels in categorical column\n",
    "d={\"ELEC\":0,\"GAZ\":1}\n",
    "invoice_train['counter_type']=invoice_train['counter_type'].map(d)\n",
    "invoice_test['counter_type']=invoice_test['counter_type'].map(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NldYnn0GWHAk"
   },
   "outputs": [],
   "source": [
    "#convert categorical columns to int for model\n",
    "client_train['client_catg'] = client_train['client_catg'].astype(int)\n",
    "client_train['disrict'] = client_train['disrict'].astype(int)\n",
    "\n",
    "client_test['client_catg'] = client_test['client_catg'].astype(int)\n",
    "client_test['disrict'] = client_test['disrict'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Bt7LJHhyXJji"
   },
   "outputs": [],
   "source": [
    "def aggregate_by_client_id(invoice_data):\n",
    "    aggs = {}\n",
    "    aggs['consommation_level_1'] = ['mean']\n",
    "    aggs['consommation_level_2'] = ['mean']\n",
    "    aggs['consommation_level_3'] = ['mean']\n",
    "    aggs['consommation_level_4'] = ['mean']\n",
    "\n",
    "    agg_trans = invoice_data.groupby(['client_id']).agg(aggs)\n",
    "    agg_trans.columns = ['_'.join(col).strip() for col in agg_trans.columns.values]\n",
    "    agg_trans.reset_index(inplace=True)\n",
    "\n",
    "    df = (invoice_data.groupby('client_id')\n",
    "            .size()\n",
    "            .reset_index(name='{}transactions_count'.format('1')))\n",
    "    return pd.merge(df, agg_trans, on='client_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8gUjxEmKWZjp"
   },
   "outputs": [],
   "source": [
    "#group invoice data by client_id\n",
    "agg_train = aggregate_by_client_id(invoice_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "beXOMOEMW97l"
   },
   "outputs": [],
   "source": [
    "#merge aggregate data with client dataset\n",
    "train = pd.merge(client_train,agg_train, on='client_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "y6SFawOVXFDX"
   },
   "outputs": [],
   "source": [
    "#aggregate test set\n",
    "agg_test = aggregate_by_client_id(invoice_test)\n",
    "test = pd.merge(client_test,agg_test, on='client_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BwYBcKk8X3mr"
   },
   "outputs": [],
   "source": [
    "#drop redundant columns\n",
    "sub_client_id = test['client_id']\n",
    "drop_columns = ['client_id', 'creation_date']\n",
    "\n",
    "for col in drop_columns:\n",
    "    if col in train.columns:\n",
    "        train.drop([col], axis=1, inplace=True)\n",
    "    if col in test.columns:\n",
    "        test.drop([col], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58069, 8)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJ3KlVOCYfnZ"
   },
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"XGBoost\": XGBClassifier(),\n",
    "    \"CatBoost\": CatBoostClassifier(verbose=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store metrics for each model\n",
    "model_metrics = {}\n",
    "\n",
    "# dictionary to store trained models\n",
    "trained_models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "X = train.drop(columns=['target'])\n",
    "y = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform train test split first\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define k-fold cross-validation\n",
    "k = 5\n",
    "stkfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for imbalance\n",
    "target_distribution = train['target'].value_counts(normalize=True)\n",
    "print(\"Target Variable Distribution:\")\n",
    "print(target_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum time for training in seconds\n",
    "max_training_time = 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle imbalance using SMOTE\n",
    "#smote = SMOTE(random_state=42)\n",
    "#X_resampled, y_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "class TimeoutError(Exception):\n",
    "    pass\n",
    "\n",
    "def timeout(seconds=1, error_message=\"Timeout occurred\"):\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            result = [TimeoutError(error_message)]\n",
    "            def target():\n",
    "                result[0] = func(*args, **kwargs)\n",
    "            thread = threading.Thread(target=target)\n",
    "            thread.start()\n",
    "            thread.join(seconds)\n",
    "            if thread.is_alive():\n",
    "                thread.join()  # Ensures that the thread is terminated\n",
    "                raise TimeoutError(error_message)\n",
    "            return result[0]\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest...\n",
      "    Elapsed Time: 196.16 seconds\n",
      "    Average Accuracy: 0.9422\n",
      "\n",
      "    Precision: 0.2222\n",
      "    Recall: 0.0141\n",
      "    F1 Score: 0.0265\n",
      "    ROC AUC: 0.5055\n",
      "\n",
      "Training Logistic Regression...\n",
      "    Elapsed Time: 6.40 seconds\n",
      "    Average Accuracy: 0.9441\n",
      "\n",
      "    Precision: 0.2000\n",
      "    Recall: 0.0019\n",
      "    F1 Score: 0.0038\n",
      "    ROC AUC: 0.5007\n",
      "\n",
      "Training Decision Tree...\n",
      "    Elapsed Time: 6.39 seconds\n",
      "    Average Accuracy: 0.8955\n",
      "\n",
      "    Precision: 0.1180\n",
      "    Recall: 0.1304\n",
      "    F1 Score: 0.1239\n",
      "    ROC AUC: 0.5354\n",
      "\n",
      "Training Gradient Boosting...\n",
      "    Elapsed Time: 182.12 seconds\n",
      "    Average Accuracy: 0.9445\n",
      "\n",
      "    Precision: 0.1667\n",
      "    Recall: 0.0006\n",
      "    F1 Score: 0.0013\n",
      "    ROC AUC: 0.5002\n",
      "\n",
      "Training KNN...\n",
      "    Elapsed Time: 26.28 seconds\n",
      "    Average Accuracy: 0.9412\n",
      "\n",
      "    Precision: 0.1268\n",
      "    Recall: 0.0115\n",
      "    F1 Score: 0.0211\n",
      "    ROC AUC: 0.5033\n",
      "\n",
      "Training Naive Bayes...\n",
      "    Elapsed Time: 0.52 seconds\n",
      "    Average Accuracy: 0.9340\n",
      "\n",
      "    Precision: 0.1842\n",
      "    Recall: 0.0537\n",
      "    F1 Score: 0.0832\n",
      "    ROC AUC: 0.5196\n",
      "\n",
      "Training XGBoost...\n",
      "    Elapsed Time: 5.38 seconds\n",
      "    Average Accuracy: 0.9440\n",
      "\n",
      "    Precision: 0.3607\n",
      "    Recall: 0.0141\n",
      "    F1 Score: 0.0271\n",
      "    ROC AUC: 0.5063\n",
      "\n",
      "Training CatBoost...\n",
      "    Elapsed Time: 166.02 seconds\n",
      "    Average Accuracy: 0.9440\n",
      "\n",
      "    Precision: 0.3000\n",
      "    Recall: 0.0077\n",
      "    F1 Score: 0.0150\n",
      "    ROC AUC: 0.5033\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        # Perform cross-validation with a timeout\n",
    "        @timeout(max_training_time)\n",
    "        def cross_validation_with_timeout():\n",
    "            return cross_val_score(model, X_train, y_train, cv=stkfold, scoring='accuracy')\n",
    "        \n",
    "        scores = cross_validation_with_timeout()\n",
    "\n",
    "        # calculate the elapsed time for training\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"    Elapsed Time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    except TimeoutError:\n",
    "        print(f\"    Training {model_name} exceeded the maximum training time of {max_training_time} seconds. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Store metrics in dictionary\n",
    "    model_metrics[model_name] = {\n",
    "        \"Accuracy\": scores.mean(),\n",
    "        \"Precision\": 0,  # Placeholder for precision\n",
    "        \"Recall\": 0,     # Placeholder for recall\n",
    "        \"F1 Score\": 0,   # Placeholder for F1 score\n",
    "        \"ROC AUC\": 0     # Placeholder for ROC AUC\n",
    "    }\n",
    "\n",
    "    print(f\"    Average Accuracy: {scores.mean():.4f}\")\n",
    "    print()\n",
    "\n",
    "    # Fit the model on the entire training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # store trained model\n",
    "    trained_models[model_name] = model\n",
    "\n",
    "    # Calculate precision, recall, F1 score, and ROC AUC using the entire training data\n",
    "    y_pred = model.predict(X_test)\n",
    "    model_metrics[model_name][\"Precision\"] = precision_score(y_test, y_pred)\n",
    "    model_metrics[model_name][\"Recall\"] = recall_score(y_test, y_pred)\n",
    "    model_metrics[model_name][\"F1 Score\"] = f1_score(y_test, y_pred)\n",
    "    model_metrics[model_name][\"ROC AUC\"] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"    Precision: {model_metrics[model_name]['Precision']:.4f}\")\n",
    "    print(f\"    Recall: {model_metrics[model_name]['Recall']:.4f}\")\n",
    "    print(f\"    F1 Score: {model_metrics[model_name]['F1 Score']:.4f}\")\n",
    "    print(f\"    ROC AUC: {model_metrics[model_name]['ROC AUC']:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize models based on performance\n",
    "categorized_models = {}\n",
    "for model_name, metrics in model_metrics.items():\n",
    "    if metrics[\"Accuracy\"] >= 0.8 and metrics[\"ROC AUC\"] >= 0.8:\n",
    "        categorized_models[model_name] = \"Great\"\n",
    "    elif metrics[\"Accuracy\"] >= 0.7 and metrics[\"ROC AUC\"] >= 0.7:\n",
    "        categorized_models[model_name] = \"Good\"\n",
    "    elif metrics[\"Accuracy\"] >= 0.6 and metrics[\"ROC AUC\"] >= 0.6:\n",
    "        categorized_models[model_name] = \"Okay\"\n",
    "    else:\n",
    "        categorized_models[model_name] = \"Nuh\"\n",
    "\n",
    "# Print categorized models\n",
    "for model_name, category in categorized_models.items():\n",
    "    print(f\"{model_name}: {category}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the models on the test data\n",
    "test_metrics = {}\n",
    "for model_name, model in trained_models.items():\n",
    "    print(f\"Evaluating {model_name} on the test data...\")\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    y_pred = model.predict(test)\n",
    "\n",
    "    # initialize test_metrics[model_name]\n",
    "    if model_name not in test_metrics:\n",
    "        test_metrics[model_name] = {}\n",
    "\n",
    "    # assign predictions to test_metrics[model_name]\n",
    "    test_metrics[model_name][\"predictions\"] = y_pred\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    # Store evaluation metrics in test_metrics[model_name]\n",
    "    test_metrics[model_name][\"Accuracy\"] = accuracy\n",
    "    test_metrics[model_name][\"Precision\"] = precision\n",
    "    test_metrics[model_name][\"Recall\"] = recall\n",
    "    test_metrics[model_name][\"F1 Score\"] = f1\n",
    "    test_metrics[model_name][\"ROC AUC\"] = roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print test metrics\n",
    "for model_name, metrics in test_metrics.items():\n",
    "    print(f\"\\n{model_name} Test Metrics:\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"{metric_name}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate submission CSV file for a model\n",
    "def generate_submission_csv(model_name, predictions):\n",
    "    submission_df = pd.DataFrame({\n",
    "        'client_id': sub_client_id,\n",
    "        'target': predictions\n",
    "    })\n",
    "    submission_df.to_csv(f'{model_name}_submission.csv', index=False)\n",
    "    print(f'Submission CSV file generated for {model_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submission CSV files for models that predicted successfully\n",
    "for model_name, metrics in test_metrics.items():\n",
    "    print(f\"{model_name}...\")\n",
    "    if 'predictions' in metrics:  # Check if predictions are available\n",
    "        generate_submission_csv(model_name, metrics['predictions'])\n",
    "    else:\n",
    "        print(f\"No predictions available for {model_name}. Skipping...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Predict on validation set\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    roc_auc = roc_auc_score(y_val, y_pred)\n",
    "    \n",
    "    # Store metrics in dictionary\n",
    "    model_metrics[model_name] = {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1,\n",
    "        \"ROC AUC\": roc_auc\n",
    "    }\n",
    "\n",
    "    print(f\"    Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"    Precision: {precision:.4f}\")\n",
    "    print(f\"    Recall: {recall:.4f}\")\n",
    "    print(f\"    F1 Score: {f1:.4f}\")\n",
    "    print(f\"    ROC AUC: {roc_auc:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize models based on performance\n",
    "categorized_models = {}\n",
    "for model_name, metrics in model_metrics.items():\n",
    "    if metrics[\"Accuracy\"] >= 0.8 and metrics[\"ROC AUC\"] >= 0.8:\n",
    "        categorized_models[model_name] = \"Great\"\n",
    "    elif metrics[\"Accuracy\"] >= 0.7 and metrics[\"ROC AUC\"] >= 0.7:\n",
    "        categorized_models[model_name] = \"Good\"\n",
    "    elif metrics[\"Accuracy\"] >= 0.6 and metrics[\"ROC AUC\"] >= 0.6:\n",
    "        categorized_models[model_name] = \"Okay\"\n",
    "    else:\n",
    "        categorized_models[model_name] = \"Nuh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print categorized models\n",
    "for model_name, category in categorized_models.items():\n",
    "    print(f\"{model_name}: {category}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print metrics of categorized models\n",
    "for model_name, category in categorized_models.items():\n",
    "    print(f\"{model_name}: {category}\")\n",
    "    if category == \"Nuh\":\n",
    "        metrics = model_metrics[model_name]\n",
    "        print(f\"    Accuracy: {metrics['Accuracy']:.4f}\")\n",
    "        print(f\"    Precision: {metrics['Precision']:.4f}\")\n",
    "        print(f\"    Recall: {metrics['Recall']:.4f}\")\n",
    "        print(f\"    F1 Score: {metrics['F1 Score']:.4f}\")\n",
    "        print(f\"    ROC AUC: {metrics['ROC AUC']:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = best_model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(\n",
    "    {\n",
    "        'client_id': sub_client_id,\n",
    "        'target': test_predict['target']\n",
    "    }\n",
    ")\n",
    "\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKsrOiePadZ7"
   },
   "source": [
    "## Make Predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "SGIjyZfmaf7C",
    "outputId": "b4bdd7a5-a496-405c-9651-ba482afd1046"
   },
   "outputs": [],
   "source": [
    "preds = model.predict(test)\n",
    "preds = pd.DataFrame(preds, columns=['target'])\n",
    "preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "mX3rv6dwaw3-",
    "outputId": "6db1fc21-c545-4b99-a6f1-9357b2ef405d"
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(\n",
    "    {\n",
    "        'client_id': sub_client_id,\n",
    "        'target': preds['target']\n",
    "    }\n",
    ")\n",
    "\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7_ANcWUboUB"
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
