{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/imgremlin/Photos/master/electricity.jpg\" width=\"1000px\"> \n",
    "# Fraud Detection in Electricity and Gas Consumption Challenge\n",
    "**by team GORNYAKI (Tsepa Oleksii and Samoshin Andriy [Ukraine, KPI, IASA])**\n",
    "\n",
    "Thanks to the organizers for this [challenge](https://zindi.africa/competitions/ai-hack-tunisia-4-predictive-analytics-challenge-1) and everyone for participating! In this notebook you will find:\n",
    "\n",
    "* importing libraries\n",
    "* basic EDA\n",
    "* feature engeneering\n",
    "* modelling\n",
    "* prediction \n",
    "* submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Importing libraries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bayesian-optimizationNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading bayesian_optimization-1.4.3-py3-none-any.whl.metadata (543 bytes)\n",
      "Requirement already satisfied: numpy>=1.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from bayesian-optimization) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from bayesian-optimization) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from bayesian-optimization) (1.2.2)\n",
      "Requirement already satisfied: colorama>=0.4.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from bayesian-optimization) (0.4.6)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (2.2.0)\n",
      "Downloading bayesian_optimization-1.4.3-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: bayesian-optimization\n",
      "Successfully installed bayesian-optimization-1.4.3\n"
     ]
    }
   ],
   "source": [
    "#pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lightgbm import LGBMClassifier\n",
    "import time\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "seed=47\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "invoice_test = pd.read_csv('invoice_test.csv',low_memory=False)\n",
    "invoice_train = pd.read_csv('invoice_train.csv',low_memory=False)\n",
    "client_test = pd.read_csv('client_test.csv',low_memory=False)\n",
    "client_train = pd.read_csv('client_train.csv',low_memory=False)\n",
    "sample_submission = pd.read_csv('submission_fraud-3.csv',low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Basic EDA</h2>\n",
    "\n",
    "We won't show full EDA, just want to attract your attention to tips which help us to reach good score.\n",
    "\n",
    "In next two cells you will find value counts according each column in train and test set. This information we'll use in feature engeneering  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = client_train.groupby(['target'])['client_id'].count()\n",
    "plt.bar(x=ds.index, height=ds.values, tick_label =[0,1])\n",
    "plt.title('target distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['disrict','region','client_catg']:\n",
    "    ds = client_train.groupby([col])['client_id'].count()\n",
    "    plt.bar(x=ds.index, height=ds.values)\n",
    "    plt.title(col+' distribution')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of missing rows in invoice_train:',invoice_train.isna().sum().sum())\n",
    "print('Number of missing rows in invoice_test:',invoice_test.isna().sum().sum(),'\\n')\n",
    "print('Number of missing rows in client_train:',client_train.isna().sum().sum())\n",
    "print('Number of missing rows in client_test:',client_test.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of unique values in invoice_train:')\n",
    "for col in invoice_train.columns:\n",
    "    print(f\"{col} - {invoice_train[col].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Feature engeneering</h2>\n",
    "\n",
    "In this part we want to explain the most powerful decision in our notebook - feature creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_change(cl, inv):\n",
    "\n",
    "    cl['client_catg'] = cl['client_catg'].astype('category')\n",
    "    cl['disrict'] = cl['disrict'].astype('category')\n",
    "    cl['region'] = cl['region'].astype('category')\n",
    "    cl['region_group'] = cl['region'].apply(lambda x: 100 if x<100 else 300 if x>300 else 200)\n",
    "    cl['creation_date'] = pd.to_datetime(cl['creation_date'])\n",
    "    \n",
    "    cl['coop_time'] = (2019 - cl['creation_date'].dt.year)*12 - cl['creation_date'].dt.month\n",
    "\n",
    "    inv['counter_type'] = inv['counter_type'].map({\"ELEC\":1,\"GAZ\":0})\n",
    "    inv['counter_statue'] = inv['counter_statue'].map({0:0,1:1,2:2,3:3,4:4,5:5,769:5,'0':0,'5':5,'1':1,'4':4,'A':0,618:5,269375:5,46:5,420:5})\n",
    "    \n",
    "    inv['invoice_date'] = pd.to_datetime(inv['invoice_date'], dayfirst=True)\n",
    "    inv['invoice_month'] = inv['invoice_date'].dt.month\n",
    "    inv['invoice_year'] = inv['invoice_date'].dt.year\n",
    "    inv['is_weekday'] = ((pd.DatetimeIndex(inv.invoice_date).dayofweek) // 5 == 1).astype(float)\n",
    "    inv['delta_index'] = inv['new_index'] - inv['old_index']\n",
    "    \n",
    "    return cl, inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 'client_catg', 'district' and 'region' were assigned as categories to use them as categorical features in lgbm (as for me, lgbm for default threats with cat features slightly better than other encoders such as catboost/target encoder)\n",
    "* 'region_group' created simply by dividing 'region' in 3 groups (we purposed that regions weren't randomly decoded)\n",
    "* 'coop_time' - amount of time since account creation in months\n",
    "* 'counter_type' was binary encoded \n",
    "* 'counter_statue' cleaned from mislabeled values\n",
    "* extracted month, year from 'invoice_date', also added binary feature - 'is_weekday'\n",
    "* not sure about any logical sense in 'delta_index', but it improved score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_25172\\2795683553.py:7: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  cl['creation_date'] = pd.to_datetime(cl['creation_date'])\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_25172\\2795683553.py:14: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  inv['invoice_date'] = pd.to_datetime(inv['invoice_date'], dayfirst=True)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_25172\\2795683553.py:7: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  cl['creation_date'] = pd.to_datetime(cl['creation_date'])\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_25172\\2795683553.py:14: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  inv['invoice_date'] = pd.to_datetime(inv['invoice_date'], dayfirst=True)\n"
     ]
    }
   ],
   "source": [
    "client_train1, invoice_train1 = feature_change(client_train, invoice_train)\n",
    "client_test1, invoice_test1 = feature_change(client_test, invoice_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_feature(invoice, client_df, agg_stat):\n",
    "    \n",
    "    invoice['delta_time'] = invoice.sort_values(['client_id','invoice_date']).groupby('client_id')['invoice_date'].diff().dt.days.reset_index(drop=True)\n",
    "    agg_trans = invoice.groupby('client_id')[agg_stat+['delta_time']].agg(['mean','std','min','max'])\n",
    "    \n",
    "    agg_trans.columns = ['_'.join(col).strip() for col in agg_trans.columns.values]\n",
    "    agg_trans.reset_index(inplace=True)\n",
    "\n",
    "    df = invoice.groupby('client_id').size().reset_index(name='transactions_count')\n",
    "    agg_trans = pd.merge(df, agg_trans, on='client_id', how='left')\n",
    "    \n",
    "    weekday_avg = invoice.groupby('client_id')[['is_weekday']].agg(['mean'])\n",
    "    weekday_avg.columns = ['_'.join(col).strip() for col in weekday_avg.columns.values]\n",
    "    weekday_avg.reset_index(inplace=True)\n",
    "    client_df = pd.merge(client_df, weekday_avg, on='client_id', how='left')\n",
    "    \n",
    "    full_df = pd.merge(client_df, agg_trans, on='client_id', how='left')\n",
    "    \n",
    "    full_df['invoice_per_cooperation'] = full_df['transactions_count'] / full_df['coop_time']\n",
    "    \n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* created some aggregation features (min/max/mean/std) over continious columns per every client\n",
    "* added 'delta_time' - amount of time between invoices for each user\n",
    "* created 'invoice_per_cooperation' - number of transactions per some amount of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_stat_columns = [\n",
    " 'tarif_type',\n",
    " 'counter_number',\n",
    " 'counter_statue',\n",
    " 'counter_code',\n",
    " 'reading_remarque',\n",
    " 'consommation_level_1',\n",
    " 'consommation_level_2',\n",
    " 'consommation_level_3',\n",
    " 'consommation_level_4',\n",
    " 'old_index',\n",
    " 'new_index',\n",
    " 'months_number',\n",
    " 'counter_type',\n",
    " 'invoice_month',\n",
    " 'invoice_year',\n",
    " 'delta_index'\n",
    "]\n",
    "\n",
    "train_df1 = agg_feature(invoice_train1, client_train1, agg_stat_columns)\n",
    "test_df1 = agg_feature(invoice_test1, client_test1, agg_stat_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_features(df):\n",
    "    \n",
    "    for col in agg_stat_columns:\n",
    "        df[col+'_range'] = df[col+'_max'] - df[col+'_min']\n",
    "        df[col+'_max_mean'] = df[col+'_max']/df[col+'_mean']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we created statistical 'max_mean' and 'range' features which noticeably improved score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2 = new_features(train_df1)\n",
    "test_df2 = new_features(test_df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's review how many features did we create:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of columns:  29\n",
      "Number of columns now:  111\n"
     ]
    }
   ],
   "source": [
    "print('Initial number of columns: ', len(client_train.columns)+len(invoice_train.columns))\n",
    "print('Number of columns now: ', len(train_df2.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop(df):\n",
    "\n",
    "    col_drop = ['client_id', 'creation_date']\n",
    "    for col in col_drop:\n",
    "        df.drop([col], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we created really a lot of features and sure, not all of them were usefull, so we dropped some unnessesary columns in next few cells\n",
    "* 'drop_col' array was made after using our own backward feature selection function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = drop(train_df2)\n",
    "test_df = drop(test_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df['target']\n",
    "X = train_df.drop('target',axis=1)\n",
    "\n",
    "feature_name = X.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_col=['reading_remarque_max','counter_statue_min','counter_type_min','counter_type_max','counter_type_range',\n",
    "          'tarif_type_max', 'delta_index_min', 'consommation_level_4_mean']\n",
    "\n",
    "X = X.drop(drop_col, axis=1)\n",
    "test_df = test_df.drop(drop_col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Bayesian Optimization\n",
    "def lgbm_cv(n_estimators, num_leaves, max_depth, learning_rate, min_split_gain, feature_fraction, bagging_freq):\n",
    "    params = {\n",
    "        'n_estimators': int(n_estimators),\n",
    "        'num_leaves': int(num_leaves),\n",
    "        'max_depth': int(max_depth),\n",
    "        'learning_rate': learning_rate,\n",
    "        'min_split_gain': min_split_gain,\n",
    "        'feature_fraction': feature_fraction,\n",
    "        'bagging_freq': int(bagging_freq),\n",
    "        'verbose': -1,\n",
    "        'random_state': seed\n",
    "    }\n",
    "\n",
    "    # Perform cross-validation\n",
    "    stkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    scores = []\n",
    "    for train_idx, valid_idx in stkfold.split(X, y):\n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = y[train_idx], y[valid_idx]\n",
    "        \n",
    "        model = LGBMClassifier(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict_proba(X_valid)[:, 1]\n",
    "        score = roc_auc_score(y_valid, preds)\n",
    "        scores.append(score)\n",
    "\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter space for Bayesian Optimization\n",
    "pbounds = {\n",
    "    'n_estimators': (200, 1000),\n",
    "    'num_leaves': (2, 512),\n",
    "    'max_depth': (2, 128),\n",
    "    'learning_rate': (0.001, 0.15),\n",
    "    'min_split_gain': (0.001, 0.1),\n",
    "    'feature_fraction': (0.1, 1.0),\n",
    "    'bagging_freq': (1, 10)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Bayesian Optimization\n",
    "optimizer = BayesianOptimization(\n",
    "    f=lgbm_cv,\n",
    "    pbounds=pbounds,\n",
    "    random_state=seed,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | learni... | max_depth | min_sp... | n_esti... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.8799   \u001b[0m | \u001b[0m9.48     \u001b[0m | \u001b[0m0.7705   \u001b[0m | \u001b[0m0.04081  \u001b[0m | \u001b[0m47.59    \u001b[0m | \u001b[0m0.05311  \u001b[0m | \u001b[0m637.5    \u001b[0m | \u001b[0m133.9    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.884    \u001b[0m | \u001b[0m2.572    \u001b[0m | \u001b[0m0.4246   \u001b[0m | \u001b[0m0.02189  \u001b[0m | \u001b[0m51.02    \u001b[0m | \u001b[0m0.04772  \u001b[0m | \u001b[0m975.1    \u001b[0m | \u001b[0m76.24    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.8819   \u001b[0m | \u001b[0m5.628    \u001b[0m | \u001b[0m0.5749   \u001b[0m | \u001b[0m0.04648  \u001b[0m | \u001b[0m22.12    \u001b[0m | \u001b[0m0.06009  \u001b[0m | \u001b[0m283.0    \u001b[0m | \u001b[0m291.7    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.8749   \u001b[0m | \u001b[0m4.477    \u001b[0m | \u001b[0m0.1762   \u001b[0m | \u001b[0m0.08474  \u001b[0m | \u001b[0m83.66    \u001b[0m | \u001b[0m0.06653  \u001b[0m | \u001b[0m351.4    \u001b[0m | \u001b[0m488.6    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.8737   \u001b[0m | \u001b[0m1.531    \u001b[0m | \u001b[0m0.8922   \u001b[0m | \u001b[0m0.1178   \u001b[0m | \u001b[0m33.81    \u001b[0m | \u001b[0m0.09278  \u001b[0m | \u001b[0m555.8    \u001b[0m | \u001b[0m194.3    \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.8758   \u001b[0m | \u001b[0m1.364    \u001b[0m | \u001b[0m0.795    \u001b[0m | \u001b[0m0.09413  \u001b[0m | \u001b[0m31.81    \u001b[0m | \u001b[0m0.0832   \u001b[0m | \u001b[0m277.5    \u001b[0m | \u001b[0m149.6    \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.8718   \u001b[0m | \u001b[0m3.503    \u001b[0m | \u001b[0m0.6294   \u001b[0m | \u001b[0m0.1305   \u001b[0m | \u001b[0m92.13    \u001b[0m | \u001b[0m0.04914  \u001b[0m | \u001b[0m843.2    \u001b[0m | \u001b[0m130.7    \u001b[0m |\n",
      "| \u001b[95m14       \u001b[0m | \u001b[95m0.8856   \u001b[0m | \u001b[95m6.469    \u001b[0m | \u001b[95m0.3713   \u001b[0m | \u001b[95m0.01824  \u001b[0m | \u001b[95m105.2    \u001b[0m | \u001b[95m0.04956  \u001b[0m | \u001b[95m816.9    \u001b[0m | \u001b[95m89.09    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.8723   \u001b[0m | \u001b[0m3.278    \u001b[0m | \u001b[0m0.8673   \u001b[0m | \u001b[0m0.1184   \u001b[0m | \u001b[0m2.238    \u001b[0m | \u001b[0m0.01966  \u001b[0m | \u001b[0m695.9    \u001b[0m | \u001b[0m23.04    \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.8745   \u001b[0m | \u001b[0m4.279    \u001b[0m | \u001b[0m0.221    \u001b[0m | \u001b[0m0.1021   \u001b[0m | \u001b[0m40.11    \u001b[0m | \u001b[0m0.05345  \u001b[0m | \u001b[0m607.3    \u001b[0m | \u001b[0m393.2    \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.8777   \u001b[0m | \u001b[0m9.567    \u001b[0m | \u001b[0m0.2597   \u001b[0m | \u001b[0m0.06126  \u001b[0m | \u001b[0m16.35    \u001b[0m | \u001b[0m0.03101  \u001b[0m | \u001b[0m398.1    \u001b[0m | \u001b[0m422.6    \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.8759   \u001b[0m | \u001b[0m8.809    \u001b[0m | \u001b[0m0.1266   \u001b[0m | \u001b[0m0.09047  \u001b[0m | \u001b[0m67.27    \u001b[0m | \u001b[0m0.05286  \u001b[0m | \u001b[0m298.6    \u001b[0m | \u001b[0m103.9    \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.8725   \u001b[0m | \u001b[0m9.653    \u001b[0m | \u001b[0m0.1119   \u001b[0m | \u001b[0m0.08682  \u001b[0m | \u001b[0m89.05    \u001b[0m | \u001b[0m0.005305 \u001b[0m | \u001b[0m657.5    \u001b[0m | \u001b[0m467.5    \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.8788   \u001b[0m | \u001b[0m7.562    \u001b[0m | \u001b[0m0.1632   \u001b[0m | \u001b[0m0.01588  \u001b[0m | \u001b[0m90.04    \u001b[0m | \u001b[0m0.01775  \u001b[0m | \u001b[0m257.2    \u001b[0m | \u001b[0m486.3    \u001b[0m |\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m0.8809   \u001b[0m | \u001b[0m9.354    \u001b[0m | \u001b[0m0.9392   \u001b[0m | \u001b[0m0.05999  \u001b[0m | \u001b[0m120.6    \u001b[0m | \u001b[0m0.09816  \u001b[0m | \u001b[0m650.9    \u001b[0m | \u001b[0m318.9    \u001b[0m |\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m0.8825   \u001b[0m | \u001b[0m7.516    \u001b[0m | \u001b[0m0.8214   \u001b[0m | \u001b[0m0.03892  \u001b[0m | \u001b[0m116.6    \u001b[0m | \u001b[0m0.003949 \u001b[0m | \u001b[0m375.2    \u001b[0m | \u001b[0m79.79    \u001b[0m |\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m0.8804   \u001b[0m | \u001b[0m6.607    \u001b[0m | \u001b[0m0.4748   \u001b[0m | \u001b[0m0.03226  \u001b[0m | \u001b[0m49.43    \u001b[0m | \u001b[0m0.01338  \u001b[0m | \u001b[0m960.4    \u001b[0m | \u001b[0m219.4    \u001b[0m |\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m0.871    \u001b[0m | \u001b[0m8.823    \u001b[0m | \u001b[0m0.1207   \u001b[0m | \u001b[0m0.009085 \u001b[0m | \u001b[0m16.54    \u001b[0m | \u001b[0m0.004129 \u001b[0m | \u001b[0m231.4    \u001b[0m | \u001b[0m261.7    \u001b[0m |\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m0.8776   \u001b[0m | \u001b[0m7.61     \u001b[0m | \u001b[0m0.5189   \u001b[0m | \u001b[0m0.06363  \u001b[0m | \u001b[0m96.71    \u001b[0m | \u001b[0m0.05408  \u001b[0m | \u001b[0m759.9    \u001b[0m | \u001b[0m58.73    \u001b[0m |\n",
      "| \u001b[0m26       \u001b[0m | \u001b[0m0.8821   \u001b[0m | \u001b[0m4.691    \u001b[0m | \u001b[0m0.3227   \u001b[0m | \u001b[0m0.04699  \u001b[0m | \u001b[0m7.027    \u001b[0m | \u001b[0m0.05105  \u001b[0m | \u001b[0m425.5    \u001b[0m | \u001b[0m46.74    \u001b[0m |\n",
      "=============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "optimizer.maximize(init_points=5, n_iter=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'bagging_freq': 6.468928523306753, 'feature_fraction': 0.3712885964713344, 'learning_rate': 0.018241924681123255, 'max_depth': 105.182533635031, 'min_split_gain': 0.04956066394995558, 'n_estimators': 816.9058482514012, 'num_leaves': 89.0885086859329}\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the best hyperparameters\n",
    "best_params = optimizer.max['params']\n",
    "print(\"Best hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    'bagging_freq': 6.767701100293832, 'feature_fraction': 0.3899871532625311, \n",
    "    'learning_rate': 0.028638626573307676, 'max_depth': 117.56638717872684, \n",
    "    'min_split_gain': 0.02782115953410223, 'n_estimators': 418.838313277975, \n",
    "    'num_leaves': 488.75046802325676\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'bagging_freq': 6, 'feature_fraction': 0.3712885964713344, 'learning_rate': 0.018241924681123255, 'max_depth': 105, 'min_split_gain': 0.04956066394995558, 'n_estimators': 816, 'num_leaves': 89}\n"
     ]
    }
   ],
   "source": [
    "# Convert certain hyperparameters to integer type\n",
    "best_params['n_estimators'] = int(best_params['n_estimators'])\n",
    "best_params['num_leaves'] = int(best_params['num_leaves'])\n",
    "best_params['max_depth'] = int(best_params['max_depth'])\n",
    "best_params['bagging_freq'] = int(best_params['bagging_freq'])\n",
    "\n",
    "print(\"Best hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model using the best hyperparameters\n",
    "model = LGBMClassifier(**best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "stkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc(X, y, model, cv):\n",
    "    res=[]\n",
    "    local_probs=pd.DataFrame()\n",
    "    probs = pd.DataFrame()\n",
    "\n",
    "    for i, (tdx, vdx) in enumerate(cv.split(X, y)):\n",
    "        X_train, X_valid, y_train, y_valid = X.iloc[tdx], X.iloc[vdx], y[tdx], y[vdx]\n",
    "        model.fit(X_train, y_train,\n",
    "                 eval_set=[(X_train, y_train), (X_valid, y_valid)])\n",
    "        \n",
    "        preds = model.predict_proba(X_valid)\n",
    "        oof_predict = model.predict_proba(test_df)\n",
    "        local_probs['fold_%i'%i] = oof_predict[:,1]\n",
    "        res.append(roc_auc_score(y_valid, preds[:,1]))\n",
    "\n",
    "    print('ROC AUC:', round(np.mean(res), 6))    \n",
    "    local_probs['res'] = local_probs.mean(axis=1)\n",
    "    probs['target'] = local_probs['res']\n",
    "    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.3712885964713344, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3712885964713344\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3712885964713344, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3712885964713344\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 6053, number of negative: 102341\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.120325 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20092\n",
      "[LightGBM] [Info] Number of data points in the train set: 108394, number of used features: 100\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3712885964713344, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3712885964713344\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.055843 -> initscore=-2.827756\n",
      "[LightGBM] [Info] Start training from score -2.827756\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3712885964713344, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3712885964713344\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3712885964713344, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3712885964713344\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3712885964713344, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3712885964713344\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3712885964713344, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3712885964713344\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 6053, number of negative: 102341\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.125552 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20076\n",
      "[LightGBM] [Info] Number of data points in the train set: 108394, number of used features: 100\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3712885964713344, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3712885964713344\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.055843 -> initscore=-2.827756\n",
      "[LightGBM] [Info] Start training from score -2.827756\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3712885964713344, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3712885964713344\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3712885964713344, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3712885964713344\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3712885964713344, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3712885964713344\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3712885964713344, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3712885964713344\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 6052, number of negative: 102342\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.138302 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20092\n",
      "[LightGBM] [Info] Number of data points in the train set: 108394, number of used features: 100\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3712885964713344, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3712885964713344\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.055833 -> initscore=-2.827931\n",
      "[LightGBM] [Info] Start training from score -2.827931\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3712885964713344, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3712885964713344\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3712885964713344, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3712885964713344\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3712885964713344, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3712885964713344\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3712885964713344, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3712885964713344\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 6053, number of negative: 102342\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.125265 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20082\n",
      "[LightGBM] [Info] Number of data points in the train set: 108395, number of used features: 100\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3712885964713344, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3712885964713344\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.055842 -> initscore=-2.827766\n",
      "[LightGBM] [Info] Start training from score -2.827766\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3712885964713344, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3712885964713344\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3712885964713344, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3712885964713344\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3712885964713344, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3712885964713344\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3712885964713344, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3712885964713344\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 6053, number of negative: 102342\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.126193 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20092\n",
      "[LightGBM] [Info] Number of data points in the train set: 108395, number of used features: 100\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3712885964713344, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3712885964713344\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.055842 -> initscore=-2.827766\n",
      "[LightGBM] [Info] Start training from score -2.827766\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3712885964713344, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3712885964713344\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3712885964713344, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3712885964713344\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "ROC AUC: 0.884277\n",
      "CPU times: total: 14min 29s\n",
      "Wall time: 4min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "probs = calc(X, y, model, stkfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Modelling</h2>\n",
    "\n",
    "* we used [optuna](https://optuna.org/) for hyperparameters tuning\n",
    "* it was performed with respect to StratifiedKFold cross validation on 5 folds\n",
    "* you can check parameters for tuning and their final values in cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna import Trial\n",
    "import gc\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "\n",
    "category_cols = ['disrict', 'client_catg', 'region']\n",
    "\n",
    "def objective(trial:Trial):\n",
    "    \n",
    "    gc.collect()\n",
    "    models=[]\n",
    "    validScore=0\n",
    "   \n",
    "    model,log = fitLGBM(trial,X,y)\n",
    "    \n",
    "    models.append(model)\n",
    "    gc.collect()\n",
    "    validScore+=log\n",
    "    validScore/=len(models)\n",
    "    \n",
    "    return validScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitLGBM(trial,X, y):\n",
    "    \n",
    "    params={\n",
    "      'n_estimators':trial.suggest_int('n_estimators', 0, 1000), \n",
    "      'num_leaves':trial.suggest_int('num_leaves', 2, 512),\n",
    "      'max_depth':trial.suggest_int('max_depth', 2, 128),\n",
    "      'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 0.15),\n",
    "      'min_split_gain': trial.suggest_loguniform('min_split_gain', 0.001, 0.1),\n",
    "      'feature_fraction':trial.suggest_uniform('feature_fraction',0.1, 1.0),\n",
    "      'bagging_freq':trial.suggest_int('bagging_freq',0.1,10),\n",
    "      'verbosity': -1,\n",
    "      'random_state':seed\n",
    "            }\n",
    "    stkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    model = LGBMClassifier(**params)\n",
    "    \n",
    "    res=[]\n",
    "    for i, (tdx, vdx) in enumerate(stkfold.split(X, y)):\n",
    "        X_train, X_valid, y_train, y_valid = X.iloc[tdx], X.iloc[vdx], y[tdx], y[vdx]\n",
    "        model.fit(X_train, y_train,\n",
    "                 eval_set=[(X_train, y_train), (X_valid, y_valid)])\n",
    "        preds = model.predict_proba(X_valid)\n",
    "        res.append(roc_auc_score(y_valid, preds[:,1]))\n",
    "    \n",
    "    err = np.mean(res)\n",
    "    \n",
    "    return model, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tunning with Optuna\n",
    "study = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
    "study.optimize(objective, timeout=60*60*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the best hyperparameters\n",
    "best_params = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model using the best hyperparameters\n",
    "model = LGBMClassifier(**best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''model = LGBMClassifier(random_state=seed, n_estimators=830,num_leaves=454, max_depth=61,\n",
    "                       learning_rate=0.006910869038433314, min_split_gain=0.00667926424629105, \n",
    "                       feature_fraction=0.3764303138879782, bagging_freq=8, early_stopping_rounds=30,\n",
    "                 verbose=-1)'''\n",
    "\n",
    "stkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "def calc(X, y, model, cv):\n",
    "    res=[]\n",
    "    local_probs=pd.DataFrame()\n",
    "    probs = pd.DataFrame()\n",
    "\n",
    "    for i, (tdx, vdx) in enumerate(cv.split(X, y)):\n",
    "        X_train, X_valid, y_train, y_valid = X.iloc[tdx], X.iloc[vdx], y[tdx], y[vdx]\n",
    "        model.fit(X_train, y_train,\n",
    "                 eval_set=[(X_train, y_train), (X_valid, y_valid)])\n",
    "        \n",
    "        preds = model.predict_proba(X_valid)\n",
    "        oof_predict = model.predict_proba(test_df)\n",
    "        local_probs['fold_%i'%i] = oof_predict[:,1]\n",
    "        res.append(roc_auc_score(y_valid, preds[:,1]))\n",
    "\n",
    "    print('ROC AUC:', round(np.mean(res), 6))    \n",
    "    local_probs['res'] = local_probs.mean(axis=1)\n",
    "    probs['target'] = local_probs['res']\n",
    "    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Prediction and submission</h2>\n",
    "\n",
    "In the next few cells you can see our local cross validation which almost match  LB score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "probs = calc(X, y, model, stkfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "        \"client_id\": sample_submission[\"client_id\"],\n",
    "        \"target\": probs['target']\n",
    "    })\n",
    "submission.to_csv('submission-1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up, at the time of publication of the notebook, we got 4th place in this competition!Thank you for watching, waiting your comments!\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/imgremlin/Photos/master/lb.png\" width=\"700\"> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
